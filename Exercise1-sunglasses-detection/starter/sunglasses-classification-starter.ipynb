{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34dbcc97",
   "metadata": {},
   "source": [
    "## Imports\n",
    "- Sets up Keras Core backend to use Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713df5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# need to define backend before importing Keras\n",
    "# to change a backend, you will have to restart the kernel\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras_core as keras\n",
    "from keras_core import layers\n",
    "from keras_core import ops\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import keras_cv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af75c4",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The data is organized under the `./../sunglasses-dataset` folder with one subfolder per class (`sunglasses` and `no_sunglasses`).\n",
    "```\n",
    "sunglasses-dataset\n",
    "├── sunglasses\n",
    "│   ├── phoebe_left_angry_sunglasses_2.pgm\n",
    "│   ├── phoebe_right_sad_sunglasses.pgm\n",
    "│   ...\n",
    "├── no_sunglasses\n",
    "│   ├── mitchell_straight_happy_open_4.pgm\n",
    "│   ├── phoebe_straight_neutral_open_4.pgm\n",
    "│   ├── phoebe_up_happy_open_2.pgm\n",
    "├── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb5d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and automatically organize the images into classes based on folder structure\n",
    "train_dataset, validation_dataset = keras.utils.image_dataset_from_directory(\n",
    "    directory='./../sunglasses-dataset/',\n",
    "    labels=\"inferred\",\n",
    "    label_mode='categorical',\n",
    "    # Note: you can change the image_size and batch_size to tune your model training process\n",
    "    batch_size=32,\n",
    "    image_size=(64, 64),\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "# NOTE: there is only training and validation set here, we omit the test dataset for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6614e6a",
   "metadata": {},
   "source": [
    "## Visualize Dataset\n",
    "Viewing the images within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_grid(images, grid=3, title=None):\n",
    "    fig, axes = plt.subplots(grid,grid, figsize=(grid*2,grid*2))\n",
    "    for i in range(grid):\n",
    "        for j in range(grid):\n",
    "            if i*grid + j < len(images):\n",
    "                axes[i][j].imshow(images[i*grid + j].astype('uint8'))\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "X = np.concatenate([x for x, y in train_dataset], axis=0)\n",
    "plot_image_grid(X, title=\"training_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c0a18b",
   "metadata": {},
   "source": [
    "## Build Classification Model\n",
    "Image classification problem with require multiple `layers.Conv2D()`, a flattening layer like `layers.Flatten()`, and a `layers.Dense()` to coerce the output to be a probability distribution over two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a96e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # TODO: define the convolutional layers, flatten layers, and dense layers\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc6ad0",
   "metadata": {},
   "source": [
    "## Train Classification Model\n",
    "Define the optimizer and loss to use for the training process with `model.compile()`, and run the training loop on the dataset with `model.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b113ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=, #TODO: define the optimizer\n",
    "    loss=, #TODO: define the loss\n",
    "    metrics=['accuracy'], # List of metrics to monitor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa8d16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Should be able to get at least 80% for validation accuracy\n",
    "history = model.fit(\n",
    "    # TODO: pass in the train_dataset\n",
    "    # TODO: pass in the validation_dataset through the parameter validation_data\n",
    "    # TODO: define the number of epochs to run for\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "NOTE: defining the number of epochs tells the model how closely you want it to\n",
    "fit to the training data. Too many epochs and the model will overfit on the training data\n",
    "and not be able to generalize. Too few epochs and the model will underfit, and would not\n",
    "have learnt enough patterns from the dataset to be useful. Try out a couple values for the\n",
    "epochs so that you can get validation accuracy above 80%. The optimal value should be in \n",
    "the range (5-50)\n",
    "\n",
    "NOTE: everytime you redefine the hyperparameters to the model (e.g. epochs), you will have \n",
    "to run all the cells starting from where you instantiate the model (model = keras.Sequential(...)), \n",
    "in order to make sure you have a fresh model rather than a partially trained one.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the training history through a pandas.DataFrame\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot accuracy and loss (training history) of the run\n",
    "def plot_history(history_df):\n",
    "    fig, axes = plt.subplots(2,1, sharex=True)\n",
    "    sns.lineplot(data=history_df[[\"accuracy\", \"val_accuracy\"]], ax=axes[0]).set(\n",
    "        title=\"accuracy over iterations of training\",\n",
    "        xlabel=\"iterations\",\n",
    "        ylabel=\"accuracy\"\n",
    "    )\n",
    "    sns.lineplot(data=history_df[[\"loss\", \"val_loss\"]], ax=axes[1]).set(\n",
    "        title=\"loss over iterations of training\",\n",
    "        xlabel=\"iterations\",\n",
    "        ylabel=\"loss\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "plot_history(history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ee13a",
   "metadata": {},
   "source": [
    "## Analyze the Model\n",
    "Running `model.evaluate()`, plotting a confusion matrix, and visualizing incorrectly classified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation dataset \n",
    "# a.k.a. running the trained model without updating the weights\n",
    "\n",
    "# TODO: run model.evaluate() on the validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting code for confusion matrix\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    labels = np.unique(y_true)\n",
    "    matrix = np.zeros((len(labels), len(labels)))\n",
    "    for i, label_true in enumerate(labels):\n",
    "        for j, label_pred in enumerate(labels):\n",
    "            matrix[i][j] = np.count_nonzero((y_true == label_true) & (y_pred == label_pred))\n",
    "    return matrix\n",
    "        \n",
    "def get_confusion_matrix(model, dataset):\n",
    "    y_true = np.argmax(np.concatenate([y for x, y in dataset], axis=0), axis=1)\n",
    "    y_pred = np.argmax(model.predict(dataset), axis=1)\n",
    "    confusion_mat = confusion_matrix(y_true, y_pred)\n",
    "    confusion_df = pd.DataFrame(\n",
    "        confusion_mat, \n",
    "        columns=[\"no_sunglasses\", \"sunglasses\"], \n",
    "        index=[\"no_sunglasses\", \"sunglasses\"]\n",
    "    )\n",
    "    sns.heatmap(confusion_df, annot=True).set(\n",
    "        title=\"Confusion Matrix\",\n",
    "        xlabel=\"Predictions\",\n",
    "        ylabel=\"Ground Truth\"\n",
    "    )\n",
    "get_confusion_matrix(model, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b84020",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(np.concatenate([y for x, y in validation_dataset], axis=0), axis=1)\n",
    "X = np.concatenate([x for x, y in validation_dataset], axis=0)\n",
    "y_pred = np.argmax(model.predict(validation_dataset), axis=1)\n",
    "predictions = list(zip(X, y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2fb6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted to be wearing sunglasses but isn't wearing sunglasses\n",
    "predictions1 = [image for image, y_true, y_pred in predictions if y_true == 0 and y_pred == 1]\n",
    "plot_image_grid(predictions1, grid=2, title=\"Predicted sunglasses but no sunglasses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0834d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted to not have sunglasses sunglasses but is wearing sunglasses\n",
    "predictions2 = [image for image, y_true, y_pred in predictions if y_true == 1 and y_pred == 0]\n",
    "plot_image_grid(predictions2, grid=2, title=\"Predicted no sunglasses but has sunglasses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3afac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity",
   "language": "python",
   "name": "udacity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
